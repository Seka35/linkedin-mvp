# üîç Debug du Scraping sur VPS

## Probl√®me
Le scraping retourne 0 r√©sultats alors que l'application fonctionne.

## V√©rifications √† faire

### 1. V√©rifier les logs de l'application
```bash
# Sur le VPS
docker-compose logs app --tail 100

# Chercher les erreurs de scraping
docker-compose logs app | grep -i "error\|exception\|apify\|serp"
```

### 2. V√©rifier que le .env est bien mont√©
```bash
# Entrer dans le conteneur
docker exec -it linkedin_bot_app bash

# V√©rifier si le .env existe
ls -la /app/.env

# Voir le contenu (masquer les vraies cl√©s)
cat /app/.env | grep -v "PASSWORD\|KEY\|SECRET"

# Sortir
exit
```

### 3. V√©rifier les variables d'environnement
```bash
docker exec -it linkedin_bot_app env | grep -E "APIFY|SERP|OPENROUTER"
```

## Solutions possibles

### Solution 1 : Le .env n'est pas mont√©
Le probl√®me est que le `.env` n'est **pas copi√© dans le conteneur** car il est dans `.dockerignore`.

**Fix** : Ajouter un volume dans docker-compose.yml
```yaml
volumes:
  - ./data:/app/data
  - ./logs:/app/logs
  - ./.env:/app/.env  # ‚Üê Ajouter cette ligne
```

### Solution 2 : Les API keys sont invalides
V√©rifier que les vraies cl√©s API sont dans `/opt/linkedin-mvp/.env` sur le VPS :
- `APIFY_API_KEY`
- `SERP_API_KEY`
- `OPENROUTER_KEY`

### Solution 3 : Passer les variables via docker-compose
Au lieu de monter le .env, passer les variables directement dans docker-compose.yml :
```yaml
environment:
  - FLASK_ENV=production
  - PYTHONUNBUFFERED=1
  - TZ=Europe/Paris
  - APIFY_API_KEY=${APIFY_API_KEY}
  - SERP_API_KEY=${SERP_API_KEY}
  - OPENROUTER_KEY=${OPENROUTER_KEY}
```

## Commandes de test

```bash
# Tester l'API Apify depuis le conteneur
docker exec -it linkedin_bot_app python -c "
import os
from dotenv import load_dotenv
load_dotenv()
print('APIFY_API_KEY:', os.getenv('APIFY_API_KEY', 'NOT FOUND'))
print('SERP_API_KEY:', os.getenv('SERP_API_KEY', 'NOT FOUND'))
"
```
